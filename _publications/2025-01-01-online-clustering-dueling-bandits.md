---
title: "Online Clustering of Dueling Bandits"
collection: publications
category: conferences
permalink: /publication/2025-01-01-online-clustering-dueling-bandits
excerpt: 'The contextual multi-armed bandit (MAB) is a widely used framework for problems requiring sequential decision-making under uncertainty, such as recommendation systems. In applications involving a large number of users, the performance of contextual MAB can be significantly improved by facilitating collaboration among multiple users. This has been achieved by the clustering of bandits (CB) methods, which adaptively group the users into different clusters and achieve collaboration by allowing the users in the same cluster to share data. However, classical CB algorithms typically rely on numerical reward feedback, which may not be practical in certain real-world applications. For instance, in recommendation systems, it is more realistic and reliable to solicit preference feedback between pairs of recommended items rather than absolute rewards. To address this limitation, we introduce the first "clustering of dueling bandit algorithms" to enable collaborative decision-making based on preference feedback. We propose two novel algorithms: (1) Clustering of Linear Dueling Bandits (COLDB) which models the user reward functions as linear functions of the context vectors, and (2) Clustering of Neural Dueling Bandits (CONDB) which uses a neural network to model complex, non-linear user reward functions. Both algorithms are supported by rigorous theoretical analyses, demonstrating that user collaboration leads to improved regret bounds. Extensive empirical evaluations on synthetic and real-world datasets further validate the effectiveness of our methods, establishing their potential in real-world applications involving multiple users with preference-based feedback.'
date: 2025-01-01
venue: 'ICML 2025'
paperurl: 'https://arxiv.org/abs/2502.02079'
citation: 'Zhiyong Wang, Jiahang Sun, <b>Mingze Kong</b>, Jize Xie, Qinghua Hu, John C. S. Lui, Zhongxiang Dai. (2025). &quot;Online Clustering of Dueling Bandits.&quot; <i>Proceedings of the 42nd International Conference on Machine Learning (ICML)</i>.'
---

